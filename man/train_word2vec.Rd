% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/word2vec.R
\name{train_word2vec}
\alias{train_word2vec}
\title{Train a model by word2vec.}
\usage{
train_word2vec(train_file, output_file = "vectors.bin", vectors = 100,
  threads = 1, window = 12, classes = 0, cbow = 0, min_count = 5,
  iter = 5, force = F)
}
\arguments{
\item{train_file}{Path of a single .txt file for training. Tokens are split on spaces.}

\item{output_file}{Path of the output file.}

\item{vectors}{The number of vectors to output. Defaults to 100.
More vectors usually means more precision, but also more random error, higher memory usage, and slower operations.
Sensible choices are probably in the range 100-500.}

\item{threads}{Number of threads to run training process on.
Defaults to 1; up to the number of (virtual) cores on your machine may speed things up.}

\item{window}{The size of the window (in words) to use in training.}

\item{classes}{Number of classes for k-means clustering. Not documented/tested.}

\item{cbow}{If 1, use a continuous-bag-of-words model instead of skip-grams.
Defaults to false (recommended for newcomers).}

\item{min_count}{Minimum times a word must appear to be included in the samples.
High values help reduce model size.}

\item{iter}{Number of passes to make over the corpus in training.}

\item{force}{Whether to overwrite existing model files.}
}
\value{
A word2vec object.
}
\description{
Train a model by word2vec.
}
\details{
The word2vec tool takes a text corpus as input and produces the
word vectors as output. It first constructs a vocabulary from the
training text data and then learns vector representation of words.
The resulting word vector file can be used as features in many
natural language processing and machine learning applications.
}
\examples{
\dontrun{
model = word2vec(system.file("examples", "rfaq.txt", package = "tmcn.word2vec"))
}
}
\references{
\url{https://code.google.com/p/word2vec/}
}
\author{
Jian Li <\email{rweibo@sina.com}>, Ben Schmidt <\email{bmchmidt@gmail.com}>
}
